import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import  lxml.html as html
import re


#The url of wikipedia page
url = 'https://en.wikipedia.org/wiki/Demographics_of_Toronto_neighbourhoods'


r = requests.get(url)


#If code ==200, we have a correct response from the url.
r.status_code==200


#Scraping the website
soup = BeautifulSoup(r.text, 'lxml')


tables = soup.find_all("table",attrs = {'class':'wikitable'})
print('Number of tables', len(tables))


east_york = tables[0]


body = east_york.find_all('tr')


#Columns names are the first item of the body list
head = body[0]


#Declare empty list to keep columns names
headings = []
for item in head.find_all('th'):
    item = (item.text).rstrip('\n')
    #append the clean colummn name to headings
    headings.append(item)
print(headings)
    


all_rows = [] # will be a list for list for all rows
# Now loop for the other loops
for i in range(len(tables)):
    table = tables[i]
    body = table.find_all('tr')
    body_rows = body[1:]
    for row_num in range(len(body_rows)): # A row at a time
        row = [] # this will old entries for one row
        for row_item in body_rows[row_num].find_all("td"): #loop through all row entries
            # row_item.text removes the tags from the entries
            # the following regex is to remove \xa0 and \n and comma from row_item.text
            # xa0 encodes the flag, \n is the newline and comma separates thousands in numbers
            aa = re.sub("(\xa0)|(\n)|,","",row_item.text)
            #append aa to row - note one row entry is being appended
            row.append(aa)
        # append one row to all_rows
        all_rows.append(row)


# We can now use the data on all_rowsa and headings to make a table
# all_rows becomes our data and headings the column names
df = pd.DataFrame(data=all_rows,columns=headings)
df.head()


df.info()


#Use 



